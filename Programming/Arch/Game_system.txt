Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2011-11-17T17:38:40+08:00

====== Game system ======
Created Thursday 17 November 2011

整个游戏系统（一期工程需要的）大体由这样一些部分组成：

    客户端，运行在玩家的终端上，用一条 TCP 连接和系统通讯。它接入游戏服务器网关。

    网关，负责汇总所有客户端，大体上和我很多年前的想法一致 ，虽然会有一些实现上的小改变，但思想没有根本变化。

    客户代理(Agent) ，运行在网关的后端，在逻辑上，每个客户端都有一个 agent 负责翻译和转发客户端发来的请求，以及回应。众多 agent 可以实现在同一个进程中，也可以是多个不同的进程里。可以用脚本虚拟机的形式跑，也可以是别的形式，这些都不太重要。这一次，我们最大可能使用独立的 lua state 来实现单个 agent。

    数据服务，保存玩家数据，场景数据等。前端用自己的代码和协议同系统其它部分沟通，后端这次想采用 redis 做储存。

    场景管理器，用于管理静态场景和动态副本。

    若干场景服务器，用于玩家在里面做漫游。

网关之后的服务是相互信任的，并组成一个虚拟网络可以相互通讯。通讯底层暂时采用 zeromq ，通讯协议采用 google protobuf 。

客户端到 agent 的通讯协议与网关后各个逻辑结点之间的通讯协议将隔离，分开设计。甚至不保证采用一致的技术实现方案，以及协议设计风格。

这里的设计关键在于 Agent 的设计，大部分的工作量是围绕它而来的。

单个 Agent 的工作流程代表了项目一期的结果会展现的东西，大体上逻辑流程如下：

    等待用户认证

    把认证系统交给认证服务器认证，失败则返回 1 重试。

    从数据库获取用户数据(一期数据很少,仅有默认的用户名,用户形象) ，并转发给客户端。

    取得用户所在场景号，从场景管理器取得场景服务的位置，并申请加入场景。

    从场景服务器，同步环境。

    转发用户在场景中漫游的请求，并同步场景的实时数据。

    一旦客户端发生异常或得到推出消息，通知场景服务器离开。

这里，和历史上我们的游戏服务器设计有一个小的不同。我认为，用户的角色在场景中的位置，动作状态，甚至以后的战斗数值状态，都是属于场景数据的一部分，而不是用户数据的一部分。

用户数据中，和场景有关的只包括他当前所属的场景。由场景自己来保存角色在场景中的状态数据。简单而具体的说，类似网易历史上的西游系列，玩家的坐标是玩家的属性之一，是会在持久化时存放在玩家数据里的。经过我这两年的思考，我觉得这种设计方法是有问题的。

从我现在的直觉上来说，虚拟角色的属性不应包括角色的地理位置信息。那是角色的外在约束。而场景更应该拥有在它其中的 PC 和 NPC 的位置以及各种状态。PC 下线只应该认为是 PC 处于一种特殊状态（不被周围的 PC/NPC 所见或影响），而并没有脱离这个场景。所以我更愿意把 PC 下线定义成 detach ，而上线则是 attach 的操作。在这点上，场景，作为一个实体，拥有它自己的数据逻辑。

agent 相关的协议粗略的设计如下：

    auth 登陆认证
        user/pass 取得 userid ，返回成功或各种失败
    userdb
        用 userid 取得 avatar list
        用 avatar 取得 avatar data (包括场景名)
    scene manager
        用 场景名 取得 scene id
        用 scene id 取得场景状态，返回允许进入或各种拥堵状态
    scene
        把 avatar attach 到场景
        取得 avatar 的场景上下文
        把 avatar detach 出场景
        avatar 设置坐标,速度,行为(跑,走,站立),方向
        avatar 骑乘状态改变
        avatar 做特定动作
http://blog.codingnow.com/2011/11/dev_note_1.html

按照我们一期项目的需求，昨天我简单设计了数据库里的数据格式。数据库采用的是 Redis ，我把它看成一个远端的数据结构保存设备。它提供基本的 Key-Value 储存功能，没有层级表。如果需要两层结构，可以在 Value 里保存一组 Hashes 。

这是我第一次实战使用 Redis ，没有什么经验。不过类似的设施几年前自己实现过，区别不大。经过这几年，有了 Redis 这个开源项目，就不需要重造轮子了。但其模式还是比较熟悉的。也就是说，是按我历史经验来使用 Redis 。

一期项目需要比较简单，不打算把数据拆分到不同的数据服务器上。但为日后的拆分需求做好设计准备。以后有需要，可以按 Key 的前缀把数据分到不同的位置。例如，account 信息是最可能独立出去的，因为它和具体游戏无关。

用户系统使用 email 来做用户名，但在数据库中的唯一标识是一个 uid 。用户应该允许修改登陆名（用户很可能更换 email）。用户的身份识别是用 id 来定位的。所以，在数据库中就应该有如下几组 Key ：

    account:count id
    account:userlist set(id)
    account:email:[email] id

这里，account:userlist 对应的 value 是一个 set ，里面存放了所有存在的 user id 。用于遍历所有的 user 。这个暂时可能用不上，而且当用户量相当大的时候可能有问题。不过暂时不用考虑这么多问题，等以后改进。

account:count 是一个计数器，可以用来生成唯一 id 。

account:email:[email] 用来标示每个注册的 account 的登陆名。[email] 指登陆用 email 地址。

这里，email 内可能也存在符号 ":" ，为了回避这个问题，许多对 email 进行编码。我的方案是，将字母数字 @ . _ 之外的字符编码为 %XX 的形式。用 lua 干这件事情非常简单：

local function _encode(str)
    return string.format("%%%02X",string.byte(str))
end

function emailEncode(str)
    return string.gsub(str,"([^%w_@.])",_encode)
end

当然，解码回来也很简单

local function _decode(str)
    return string.char(tonumber(str,16))
end

function emailDecode(str)
    return string.gsub(str,"%%(%w%w)",_decode)
end

之后，就是 account 下每个 id 的数据：

    account:[id]:version number
    account:[id]:email string
    account:[id]:password string // md5(password..salt)
    account:[id]:nickname string
    account:[id]:lastlogin hashes
        ip string
        time string
    account:[id]:history list(string)
    account:[id]:available enum(open/locked/delete)

其中，密码不想保存为明文。因为任何可能的数据泄露都会导致用户的损失，我也不想任何人看到用户的密码。所以采用 md5(password .. salt) 的风格。

md5 运算前，加一个 salt 后缀，是因为单纯的文本 md5 值也是有数据库可查的。

lastlogin 下保存了用户最后一次登陆的信息，使用了一张 hashes 表，因为这些信息在未来会进一步扩充。

history 保存了用户登陆的所有历史记录，用一个 string 链表记录。

用户删除自己的账户时，不想把数据从数据库删除，只想在 available 下做一个标记。

考虑到数据库内数据结构有可能发生变化，所以加了 version 域做版本标识。

我不想让各种服务可以直接读写这份数据，所以，会单独写一个认证服务器做处理。

认证服务器提供三项服务：

    用户注册

    用户名 密码 认证 (用于 ssl 连接上的 web 服务)

    用户名 密码 挑战式认证 (用于 client 的认证服务)

下面是基本的场景服务用的数据：

    account:[id]:avatars set(id)
    avatar:count id
    avatar:[id]:version number
    avatar:[id]:account id
    avatar:[id]:scene string
    avatar:[id]:available enum(open/delete)
    avatar:[id]:data hashes
        name string
        figure string
    world:scene hashes
        [name] id
    scene:count id
    scene:[id]:name string
    scene:[id]:available enum(open/close/delete)
    scene:[id]:info hashes
        time string
        pc number
    scene:[id]:pc hashes
        [id] enum[online/offline]
    scene:[id]:pc:[id] hashes
        status string

用户账号下可以有许多游戏角色，列表放在 account:[id]:avatars 下。

每个角色也拥有一个唯一 id 。这个 id 原则上和 account id 是独立体系，但是为了人类好区格，avatar:count 的起点和 account:count 不同。

角色所在场景记录一个字符串的场景名 avatar:[id]:scene ，角色的其它各种数据放在一个 hashes 里。

所有的场景索引方在 world:scene 下。如果日后有多个世界，可以采用 world:[id]:scene 。但目前不必考虑。

scene 下面的所有 pc 的在线状态放在 scene:[id]:pc hashes 中，pc 离线也把它的 id 记录在内，只有 pc 转移场景才移除。

每个 PC 的位置状态信息记录在 scene:[id]:pc:[id] 中，第一个 id 是 scene 的 id ，第二个则是 PC 的 avatar id 。

btw. 这是一份草稿，虽然思考不周，但足够满足项目一期的需求。当然许多欠考虑的地方也并非是考虑不到，而是希望尽量简单，以满足一期需求为目的。这个日后修改的代价并不大。

最后吐槽一下 Redis 的 Windows 版。办公室的 Linux 服务器还没有装好，我暂时在 Windows 下做开发。取了一份 google 搜到的 非官方 Redis 的 Windows 版 。为了图方便，使用的是 luajit ffi 去调用 hiredis 的 dll 。一开始怎么都搞不定。建立不了 socket 连接，出错码也取不到。

对比了源代码，发现修改版把 C Struct 结构改了，前面增加了几个域，而我以 hiredis 官方标准来定义的接口。

改好后，能够正确取出出错码了。发现万恶的 Windows socks api 需要调用 WSAStartup 才可以用。而 hiredis 的 Windows 修改版居然没有去调用。让我大费周折才改好，前后折腾了一个多小时。

再吐槽一下 hiredis 的 API 设计，居然依赖 C Struct 的布局。良好的 C 库的接口设计不会这么干的吧。比如 lua ，又比如 zmq 。唉，用这种东西有点小不爽。不过比 C++ 库还是好太多了。
http://blog.codingnow.com/2011/11/dev_note_2.html

这周的工作主要是写代码。

开发计划制定好后，我们便分头写代码去了。我们希望一期早点做出可以运行的东西来，一切都从简。整体的代码量并不多，如果硬拆成很多份让很多人来做的话，估计设计拆分方案，安排工作，协调每个人写的东西这些比一个人全部实现一遍的工作量还要大的多。

所以，最终就是两个人在做。怪物公司在弄客户端的东西，蜗牛同学包干了服务器。好吧，基本没我的事了，我就是那个打酱油的，好听点说，就是设计方案。当然，事情没多少，空下来的时间也可以干活。训练自己可以找到事情做，并真的做有用的事情，还是很难的。

话说回来，我们在这么一个简单的框架下，一开始确定了要采用一些现成的技术方案，即要用到 Redis , Google Protobuffer , ZeroMQ 。

Redis 和 ZeroMQ 是我最早选的，想了很久。

采用 Redis 是因为历史上，我参与的项目都没有大规模使用 SQL 数据库的传统。这和 MMO 这种特定应用有关。在 Web 开发中，面对的用户是临时的，不依赖固定连接的。你不确定用户在不在那里，你不确定同时要面对的用户有多少。你需要从小到大，采用一种可扩展容量的方案。这个时候，成熟的 SQL 数据库方案是首选。

从软件开发角度看，数据库是 MVC 模式中的 M 。以 MVC 模式解决问题，M 如何实现，采用 SQL 方案只是一种可能，绝不是唯一选择。换个角度考虑，如果是一个桌面软件，为什么大部分的 M 却没有采用数据库，而更多的是在内存中直接构建数据结构呢？性能恐怕只是一个原因，更重要的原因是面对的用户的行为不同。

为什么 MMORPG 服务器，至少在网易历史上的多款游戏，没有使用 SQL 服务做 M 。一部分原因是，网易游戏的开发源头是 Mud ，Mudos 并没有使用 SQL 作为 M ，另一部分原因是，MMORPG 面对的，同时需要服务的用户有限。而用户需要操作的数据大部分限制在用户相关的数据体内。之外的数据体非常少。及时数据总量很大，但一层索引简单（以用户 id 为索引）。每个用户都是为它持续服务，数据易变。这种行为下，从 MVC 角度看，更接近网络应用之前的软件设计。

当然你甚至可以把 M 只在逻辑上划分出来，物理上并不切换，也就是没有独立意义上的数据库服务器。这样绝对性能最高，其实只是实现了一个简单的单机游戏，允许通过网络输入多条操作流，并把行为反馈通过网络发送回去罢了。甚至比单机游戏更简单，因为没有图形控制部分。

如果程序不出问题，机房不停电，可以一直的跑下去，不用考虑数据储存问题。数据持久化不过是为了容灾罢了。把一些结构化数据持久化到硬盘上，最简洁的方案就是写操作系统层面的文件，一定比再使用一个数据库要轻量，干净的多。

有了以上背景，就不难理解，为什么我对 Redis 天生有好感。我们并没有改变设计思路，它是一直延续下来的。Redis 帮助了我们将数据服务拆分出来。当然，MMORPG 也在发展，以上提到的用户应用环境也在逐步变化，我们在软件设计上也会跟进这些变化。这些就是后话了。

ZeroMQ 呢，我是希望有一个稳健，简洁的多进程通讯方案的基础。ZeroMQ 是不错的一个。至少比 OS 的 Socket 库要实用的多。它提供了更好的模式 。这是我最为看中的。另外，这是一个 C 接口的库，容易 binding 到不同的语言下使用。

在这个问题上，蜗牛同学是反对使用 ZeroMQ 的。对他的所有反对意见，我都持保留态度。但我尊重开发人员的意愿。毕竟许多代码不会是我自己来写。蜗牛同学希望采用 Erlang + C Driver 的方式来驱动整个框架。也就是用 Erlang 来做通讯上的数据交换。其它可能采用的开发语言，都通过 Driver 的方式插入到 Erlang 的框架中去。

我个人觉得这样做的确可行，加上蜗牛同学有好几年 Erlang 开发经验，他能担负起实现框架的责任。我不是特别喜欢这个方案是因为，Erlang 这个东西还是太庞大了。我对庞大的东西天生反感。虽然以蜗牛同学的原话说，Erlang 以及他的 OTP 能写出这么多行代码来有他的必要性。我们用 C/C++(Python/Lua/Golang 等等) + ZeroMQ 实现一个拙劣的方案出来，只会漏洞百出。

我个人是不以为然的，毕竟已经做了 10 年的网络游戏，对这个领域已经很熟悉了。对于陌生领域，我们会面对许多未知的问题；但在熟悉领域，无论怎么做，方案都不可能太拙劣。只要保持最基本的简洁，我是有信心保证可以解决 MMORPG 中的各种需求的。做出来的东西也能很稳定。关键点在于，它会足够简单，能轻松的理解实现的每一行代码。

不过争论都放在一边。我的原则是，最终采用实现者自选的方案，只要它没有明显的问题。

我们最终不采用 ZeroMQ 。

Google Protobuffer 我不是很喜欢的。但采用它是多个角度妥协的结果。其实我更愿意自定义一套协议。而只是裁剪 GPB 的功能。

我认为，GPB 在最底层的协议编码定义上，做的还是不错的。改进它是多余的。作为一种协议定义，协议描述语言也算定义的不错。但只到此为止。接下来的部分就不甚满意。

GPB 协议本身，默认也是用 GPB 本身定义编码出来的。这看起来很 Cool ，但我不甚喜欢。当然所有完备的类似协议都应该有描述自己的能力。对于描述自己这件事情来说，GPB 还是稍显复杂了。

比如，如果你不借助已经有的 GPB 代码和工具，很难解析一个 GPB 协议。就好比，如果世界上第一款 C 编译器，就是最难实现的 C 编译器。因为大部分的 C 编译器是用 C 写的，实现一个 C 编译器，就陷入了先有鸡还是先有蛋的问题。

在这类问题上，据说 Lisp 比 C 要干的漂亮。不过我还是和世界上大多数程序员一样，用 C 多一些。好吧，我们还是继续用 GPB 好了。

google 在实现和使用 GPB 的时候，默认采用了一种为每个协议，生成一组代码的方式。而不是提供一套 C/C++ 库，供其它语言做 binding 。这也是我所不喜的。或许是为了性能考虑，但总觉得别扭。如果把 GPB 换成正则表达式来看，你就能理解我的心情。

现存的大多数正则表达式的实现，都是提供一组 API ，允许使用者把需要的模式以一种人类可读的串形式，编译为另一种计算机方便处理的数据结构。当你需要的时候，使用这个数据结构，交给库，就可以匹配，替换字符串了。如果默认的选择是把正则表达式编译成 C 代码，然后你用的时候再 link 到你的项目中，恐怕用的人要疯掉了。当然，生成代码这种可以带来更高的运行性能。

唔，其实这只是怎样使用 GPB 这种协议的问题，和协议定义关系不大。可惜 google 在开源之初就给出了官方的方案，引导其它语言也如法泡制，成了 GPB 的惯用法。老实说，对于 C++ ，这么做性能是不错的（其实也未必）。换到 Python 里，就非常低下了。去年我按我的思路实现了 lua 的 protobuf 解析库 ，性能可以达到和 C++ 版本差距不到一个数量级，甚至快过 java 版。

这周的剩余时间我都在写一个纯粹的 C 版的 protobuf 库，不依靠代码生成器的。希望能够作为它语言使用 GPB 的基础。别的语言只需要做 binding 就够了。这玩意挺难写，光接口设计我就改了两版。今天终于快收工了。过两天再写一篇文章专门谈谈这个问题吧。当然，还有开源。

既然在 GPB 上花了这么多功夫，当然，采用 GPB 就是最后的决定了。
http://blog.codingnow.com/2011/11/dev_note_3.html
