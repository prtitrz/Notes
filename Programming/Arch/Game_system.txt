Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2011-11-17T17:38:40+08:00

====== Game system ======
Created Thursday 17 November 2011

整个游戏系统（一期工程需要的）大体由这样一些部分组成：

    客户端，运行在玩家的终端上，用一条 TCP 连接和系统通讯。它接入游戏服务器网关。

    网关，负责汇总所有客户端，大体上和我很多年前的想法一致 ，虽然会有一些实现上的小改变，但思想没有根本变化。

    客户代理(Agent) ，运行在网关的后端，在逻辑上，每个客户端都有一个 agent 负责翻译和转发客户端发来的请求，以及回应。众多 agent 可以实现在同一个进程中，也可以是多个不同的进程里。可以用脚本虚拟机的形式跑，也可以是别的形式，这些都不太重要。这一次，我们最大可能使用独立的 lua state 来实现单个 agent。

    数据服务，保存玩家数据，场景数据等。前端用自己的代码和协议同系统其它部分沟通，后端这次想采用 redis 做储存。

    场景管理器，用于管理静态场景和动态副本。

    若干场景服务器，用于玩家在里面做漫游。

网关之后的服务是相互信任的，并组成一个虚拟网络可以相互通讯。通讯底层暂时采用 zeromq ，通讯协议采用 google protobuf 。

客户端到 agent 的通讯协议与网关后各个逻辑结点之间的通讯协议将隔离，分开设计。甚至不保证采用一致的技术实现方案，以及协议设计风格。

这里的设计关键在于 Agent 的设计，大部分的工作量是围绕它而来的。

单个 Agent 的工作流程代表了项目一期的结果会展现的东西，大体上逻辑流程如下：

    等待用户认证

    把认证系统交给认证服务器认证，失败则返回 1 重试。

    从数据库获取用户数据(一期数据很少,仅有默认的用户名,用户形象) ，并转发给客户端。

    取得用户所在场景号，从场景管理器取得场景服务的位置，并申请加入场景。

    从场景服务器，同步环境。

    转发用户在场景中漫游的请求，并同步场景的实时数据。

    一旦客户端发生异常或得到推出消息，通知场景服务器离开。

这里，和历史上我们的游戏服务器设计有一个小的不同。我认为，用户的角色在场景中的位置，动作状态，甚至以后的战斗数值状态，都是属于场景数据的一部分，而不是用户数据的一部分。

用户数据中，和场景有关的只包括他当前所属的场景。由场景自己来保存角色在场景中的状态数据。简单而具体的说，类似网易历史上的西游系列，玩家的坐标是玩家的属性之一，是会在持久化时存放在玩家数据里的。经过我这两年的思考，我觉得这种设计方法是有问题的。

从我现在的直觉上来说，虚拟角色的属性不应包括角色的地理位置信息。那是角色的外在约束。而场景更应该拥有在它其中的 PC 和 NPC 的位置以及各种状态。PC 下线只应该认为是 PC 处于一种特殊状态（不被周围的 PC/NPC 所见或影响），而并没有脱离这个场景。所以我更愿意把 PC 下线定义成 detach ，而上线则是 attach 的操作。在这点上，场景，作为一个实体，拥有它自己的数据逻辑。

agent 相关的协议粗略的设计如下：

    auth 登陆认证
        user/pass 取得 userid ，返回成功或各种失败
    userdb
        用 userid 取得 avatar list
        用 avatar 取得 avatar data (包括场景名)
    scene manager
        用 场景名 取得 scene id
        用 scene id 取得场景状态，返回允许进入或各种拥堵状态
    scene
        把 avatar attach 到场景
        取得 avatar 的场景上下文
        把 avatar detach 出场景
        avatar 设置坐标,速度,行为(跑,走,站立),方向
        avatar 骑乘状态改变
        avatar 做特定动作
http://blog.codingnow.com/2011/11/dev_note_1.html

按照我们一期项目的需求，昨天我简单设计了数据库里的数据格式。数据库采用的是 Redis ，我把它看成一个远端的数据结构保存设备。它提供基本的 Key-Value 储存功能，没有层级表。如果需要两层结构，可以在 Value 里保存一组 Hashes 。

这是我第一次实战使用 Redis ，没有什么经验。不过类似的设施几年前自己实现过，区别不大。经过这几年，有了 Redis 这个开源项目，就不需要重造轮子了。但其模式还是比较熟悉的。也就是说，是按我历史经验来使用 Redis 。

一期项目需要比较简单，不打算把数据拆分到不同的数据服务器上。但为日后的拆分需求做好设计准备。以后有需要，可以按 Key 的前缀把数据分到不同的位置。例如，account 信息是最可能独立出去的，因为它和具体游戏无关。

用户系统使用 email 来做用户名，但在数据库中的唯一标识是一个 uid 。用户应该允许修改登陆名（用户很可能更换 email）。用户的身份识别是用 id 来定位的。所以，在数据库中就应该有如下几组 Key ：

    account:count id
    account:userlist set(id)
    account:email:[email] id

这里，account:userlist 对应的 value 是一个 set ，里面存放了所有存在的 user id 。用于遍历所有的 user 。这个暂时可能用不上，而且当用户量相当大的时候可能有问题。不过暂时不用考虑这么多问题，等以后改进。

account:count 是一个计数器，可以用来生成唯一 id 。

account:email:[email] 用来标示每个注册的 account 的登陆名。[email] 指登陆用 email 地址。

这里，email 内可能也存在符号 ":" ，为了回避这个问题，许多对 email 进行编码。我的方案是，将字母数字 @ . _ 之外的字符编码为 %XX 的形式。用 lua 干这件事情非常简单：

local function _encode(str)
    return string.format("%%%02X",string.byte(str))
end

function emailEncode(str)
    return string.gsub(str,"([^%w_@.])",_encode)
end

当然，解码回来也很简单

local function _decode(str)
    return string.char(tonumber(str,16))
end

function emailDecode(str)
    return string.gsub(str,"%%(%w%w)",_decode)
end

之后，就是 account 下每个 id 的数据：

    account:[id]:version number
    account:[id]:email string
    account:[id]:password string // md5(password..salt)
    account:[id]:nickname string
    account:[id]:lastlogin hashes
        ip string
        time string
    account:[id]:history list(string)
    account:[id]:available enum(open/locked/delete)

其中，密码不想保存为明文。因为任何可能的数据泄露都会导致用户的损失，我也不想任何人看到用户的密码。所以采用 md5(password .. salt) 的风格。

md5 运算前，加一个 salt 后缀，是因为单纯的文本 md5 值也是有数据库可查的。

lastlogin 下保存了用户最后一次登陆的信息，使用了一张 hashes 表，因为这些信息在未来会进一步扩充。

history 保存了用户登陆的所有历史记录，用一个 string 链表记录。

用户删除自己的账户时，不想把数据从数据库删除，只想在 available 下做一个标记。

考虑到数据库内数据结构有可能发生变化，所以加了 version 域做版本标识。

我不想让各种服务可以直接读写这份数据，所以，会单独写一个认证服务器做处理。

认证服务器提供三项服务：

    用户注册

    用户名 密码 认证 (用于 ssl 连接上的 web 服务)

    用户名 密码 挑战式认证 (用于 client 的认证服务)

下面是基本的场景服务用的数据：

    account:[id]:avatars set(id)
    avatar:count id
    avatar:[id]:version number
    avatar:[id]:account id
    avatar:[id]:scene string
    avatar:[id]:available enum(open/delete)
    avatar:[id]:data hashes
        name string
        figure string
    world:scene hashes
        [name] id
    scene:count id
    scene:[id]:name string
    scene:[id]:available enum(open/close/delete)
    scene:[id]:info hashes
        time string
        pc number
    scene:[id]:pc hashes
        [id] enum[online/offline]
    scene:[id]:pc:[id] hashes
        status string

用户账号下可以有许多游戏角色，列表放在 account:[id]:avatars 下。

每个角色也拥有一个唯一 id 。这个 id 原则上和 account id 是独立体系，但是为了人类好区格，avatar:count 的起点和 account:count 不同。

角色所在场景记录一个字符串的场景名 avatar:[id]:scene ，角色的其它各种数据放在一个 hashes 里。

所有的场景索引方在 world:scene 下。如果日后有多个世界，可以采用 world:[id]:scene 。但目前不必考虑。

scene 下面的所有 pc 的在线状态放在 scene:[id]:pc hashes 中，pc 离线也把它的 id 记录在内，只有 pc 转移场景才移除。

每个 PC 的位置状态信息记录在 scene:[id]:pc:[id] 中，第一个 id 是 scene 的 id ，第二个则是 PC 的 avatar id 。

btw. 这是一份草稿，虽然思考不周，但足够满足项目一期的需求。当然许多欠考虑的地方也并非是考虑不到，而是希望尽量简单，以满足一期需求为目的。这个日后修改的代价并不大。

最后吐槽一下 Redis 的 Windows 版。办公室的 Linux 服务器还没有装好，我暂时在 Windows 下做开发。取了一份 google 搜到的 非官方 Redis 的 Windows 版 。为了图方便，使用的是 luajit ffi 去调用 hiredis 的 dll 。一开始怎么都搞不定。建立不了 socket 连接，出错码也取不到。

对比了源代码，发现修改版把 C Struct 结构改了，前面增加了几个域，而我以 hiredis 官方标准来定义的接口。

改好后，能够正确取出出错码了。发现万恶的 Windows socks api 需要调用 WSAStartup 才可以用。而 hiredis 的 Windows 修改版居然没有去调用。让我大费周折才改好，前后折腾了一个多小时。

再吐槽一下 hiredis 的 API 设计，居然依赖 C Struct 的布局。良好的 C 库的接口设计不会这么干的吧。比如 lua ，又比如 zmq 。唉，用这种东西有点小不爽。不过比 C++ 库还是好太多了。
http://blog.codingnow.com/2011/11/dev_note_2.html

这周的工作主要是写代码。

开发计划制定好后，我们便分头写代码去了。我们希望一期早点做出可以运行的东西来，一切都从简。整体的代码量并不多，如果硬拆成很多份让很多人来做的话，估计设计拆分方案，安排工作，协调每个人写的东西这些比一个人全部实现一遍的工作量还要大的多。

所以，最终就是两个人在做。怪物公司在弄客户端的东西，蜗牛同学包干了服务器。好吧，基本没我的事了，我就是那个打酱油的，好听点说，就是设计方案。当然，事情没多少，空下来的时间也可以干活。训练自己可以找到事情做，并真的做有用的事情，还是很难的。

话说回来，我们在这么一个简单的框架下，一开始确定了要采用一些现成的技术方案，即要用到 Redis , Google Protobuffer , ZeroMQ 。

Redis 和 ZeroMQ 是我最早选的，想了很久。

采用 Redis 是因为历史上，我参与的项目都没有大规模使用 SQL 数据库的传统。这和 MMO 这种特定应用有关。在 Web 开发中，面对的用户是临时的，不依赖固定连接的。你不确定用户在不在那里，你不确定同时要面对的用户有多少。你需要从小到大，采用一种可扩展容量的方案。这个时候，成熟的 SQL 数据库方案是首选。

从软件开发角度看，数据库是 MVC 模式中的 M 。以 MVC 模式解决问题，M 如何实现，采用 SQL 方案只是一种可能，绝不是唯一选择。换个角度考虑，如果是一个桌面软件，为什么大部分的 M 却没有采用数据库，而更多的是在内存中直接构建数据结构呢？性能恐怕只是一个原因，更重要的原因是面对的用户的行为不同。

为什么 MMORPG 服务器，至少在网易历史上的多款游戏，没有使用 SQL 服务做 M 。一部分原因是，网易游戏的开发源头是 Mud ，Mudos 并没有使用 SQL 作为 M ，另一部分原因是，MMORPG 面对的，同时需要服务的用户有限。而用户需要操作的数据大部分限制在用户相关的数据体内。之外的数据体非常少。及时数据总量很大，但一层索引简单（以用户 id 为索引）。每个用户都是为它持续服务，数据易变。这种行为下，从 MVC 角度看，更接近网络应用之前的软件设计。

当然你甚至可以把 M 只在逻辑上划分出来，物理上并不切换，也就是没有独立意义上的数据库服务器。这样绝对性能最高，其实只是实现了一个简单的单机游戏，允许通过网络输入多条操作流，并把行为反馈通过网络发送回去罢了。甚至比单机游戏更简单，因为没有图形控制部分。

如果程序不出问题，机房不停电，可以一直的跑下去，不用考虑数据储存问题。数据持久化不过是为了容灾罢了。把一些结构化数据持久化到硬盘上，最简洁的方案就是写操作系统层面的文件，一定比再使用一个数据库要轻量，干净的多。

有了以上背景，就不难理解，为什么我对 Redis 天生有好感。我们并没有改变设计思路，它是一直延续下来的。Redis 帮助了我们将数据服务拆分出来。当然，MMORPG 也在发展，以上提到的用户应用环境也在逐步变化，我们在软件设计上也会跟进这些变化。这些就是后话了。

ZeroMQ 呢，我是希望有一个稳健，简洁的多进程通讯方案的基础。ZeroMQ 是不错的一个。至少比 OS 的 Socket 库要实用的多。它提供了更好的模式 。这是我最为看中的。另外，这是一个 C 接口的库，容易 binding 到不同的语言下使用。

在这个问题上，蜗牛同学是反对使用 ZeroMQ 的。对他的所有反对意见，我都持保留态度。但我尊重开发人员的意愿。毕竟许多代码不会是我自己来写。蜗牛同学希望采用 Erlang + C Driver 的方式来驱动整个框架。也就是用 Erlang 来做通讯上的数据交换。其它可能采用的开发语言，都通过 Driver 的方式插入到 Erlang 的框架中去。

我个人觉得这样做的确可行，加上蜗牛同学有好几年 Erlang 开发经验，他能担负起实现框架的责任。我不是特别喜欢这个方案是因为，Erlang 这个东西还是太庞大了。我对庞大的东西天生反感。虽然以蜗牛同学的原话说，Erlang 以及他的 OTP 能写出这么多行代码来有他的必要性。我们用 C/C++(Python/Lua/Golang 等等) + ZeroMQ 实现一个拙劣的方案出来，只会漏洞百出。

我个人是不以为然的，毕竟已经做了 10 年的网络游戏，对这个领域已经很熟悉了。对于陌生领域，我们会面对许多未知的问题；但在熟悉领域，无论怎么做，方案都不可能太拙劣。只要保持最基本的简洁，我是有信心保证可以解决 MMORPG 中的各种需求的。做出来的东西也能很稳定。关键点在于，它会足够简单，能轻松的理解实现的每一行代码。

不过争论都放在一边。我的原则是，最终采用实现者自选的方案，只要它没有明显的问题。

我们最终不采用 ZeroMQ 。

Google Protobuffer 我不是很喜欢的。但采用它是多个角度妥协的结果。其实我更愿意自定义一套协议。而只是裁剪 GPB 的功能。

我认为，GPB 在最底层的协议编码定义上，做的还是不错的。改进它是多余的。作为一种协议定义，协议描述语言也算定义的不错。但只到此为止。接下来的部分就不甚满意。

GPB 协议本身，默认也是用 GPB 本身定义编码出来的。这看起来很 Cool ，但我不甚喜欢。当然所有完备的类似协议都应该有描述自己的能力。对于描述自己这件事情来说，GPB 还是稍显复杂了。

比如，如果你不借助已经有的 GPB 代码和工具，很难解析一个 GPB 协议。就好比，如果世界上第一款 C 编译器，就是最难实现的 C 编译器。因为大部分的 C 编译器是用 C 写的，实现一个 C 编译器，就陷入了先有鸡还是先有蛋的问题。

在这类问题上，据说 Lisp 比 C 要干的漂亮。不过我还是和世界上大多数程序员一样，用 C 多一些。好吧，我们还是继续用 GPB 好了。

google 在实现和使用 GPB 的时候，默认采用了一种为每个协议，生成一组代码的方式。而不是提供一套 C/C++ 库，供其它语言做 binding 。这也是我所不喜的。或许是为了性能考虑，但总觉得别扭。如果把 GPB 换成正则表达式来看，你就能理解我的心情。

现存的大多数正则表达式的实现，都是提供一组 API ，允许使用者把需要的模式以一种人类可读的串形式，编译为另一种计算机方便处理的数据结构。当你需要的时候，使用这个数据结构，交给库，就可以匹配，替换字符串了。如果默认的选择是把正则表达式编译成 C 代码，然后你用的时候再 link 到你的项目中，恐怕用的人要疯掉了。当然，生成代码这种可以带来更高的运行性能。

唔，其实这只是怎样使用 GPB 这种协议的问题，和协议定义关系不大。可惜 google 在开源之初就给出了官方的方案，引导其它语言也如法泡制，成了 GPB 的惯用法。老实说，对于 C++ ，这么做性能是不错的（其实也未必）。换到 Python 里，就非常低下了。去年我按我的思路实现了 lua 的 protobuf 解析库 ，性能可以达到和 C++ 版本差距不到一个数量级，甚至快过 java 版。

这周的剩余时间我都在写一个纯粹的 C 版的 protobuf 库，不依靠代码生成器的。希望能够作为它语言使用 GPB 的基础。别的语言只需要做 binding 就够了。这玩意挺难写，光接口设计我就改了两版。今天终于快收工了。过两天再写一篇文章专门谈谈这个问题吧。当然，还有开源。

既然在 GPB 上花了这么多功夫，当然，采用 GPB 就是最后的决定了。
http://blog.codingnow.com/2011/11/dev_note_3.html


话接 开发笔记1 。我们将为每个玩家的接入提供一个 agent 服务。agent 相应玩家发送过来的数据包，然后给于反馈。

对于 agent 服务，是一个典型的包驱动模式。无论我们用 Erlang 框架，还是用 ZeroMQ 自己搭建的框架，agent 都会不会有太多的不同。它将利用一个单一的输入点获取输入，根据这些输入产生输出。这个输入点是由框架提供的。框架把 Agent 感兴趣的包发给它。

我们会有许多 agent ，在没有它感兴趣的包到来时，agent 处于挂起状态（而不是轮询输入点）。这样做可以大量节省 cpu 资源。怎样做的高效，就是框架的责任了，暂时我们不展开讨论框架的设计。

下面来看，一旦数据包进入 agent 应该怎样处理。

游戏服务逻辑复杂度很高，比起很多 web 应用来说，要复杂的多。我们不能简单的把外界来的请求都看成的独立的。把输入包设计成 REST 风格，并依赖数据服务构建这套系统基本上行不通。几乎每个包都有相关的上下文环境，就是说，输入和输入之间是有联系的。简单的把输入看成一组组 session 往往也做不到 session 间独立。最终，我把游戏服务器逻辑归纳以下的需求：

游戏逻辑由若干流程构成。比如，agent 可以看作是登陆流程和场景漫游服务流程。

每个流程中，服务器可以接收若干类型的包，每种类型的包大多可以立刻处理完毕。可以简单的认为提供了一个请求回应模式的处理机。

在处理部分数据包时，可以开启一个子流程，在子流程处理完毕前，不会收到父流程认可的数据包类型。（如果收到，即为非法逻辑）

在处理部分数据包时，也可以开启一个并行流程，这个流程和已有的流程处理逻辑共存。即，框架应根据包类型把数据包分发到不同的并行流程上。例如，在场景中漫游时，可能触发一个玩家交易的并发流程。（玩家交易行为需要多次交互确认的手续，不能一次性的完成。在交易过程中，其它如战斗、聊天的处理不可中断）

每个流程都可能因为某次处理后中断退出。继而进入后续的代码逻辑。这个中断退出行为，对于并发和非并发流程都有效。

RPC 可以看作一个简单的并发流程，即等待唯一返回包，并继续流程逻辑。

我希望能设计一个简单的 DSL 来描述我要的东西，大约像这个样子：

...1
listen :
    case message A :
        ...2
        listen :
            case message B:
                ...3
                break
            case message C:
                ...4
            case message D:
                ...5
        ...6
    case message E:
        ...7
        break
...8
listen :
    case message F:
        fork listen :
            case message G:
                ...9
            case message H:
                ...10
                break
        ...11
    case message I:
        ...12

解释一下上面这张表：

它表示，服务器启动后，会运行 ...1 这些代码，然后开始等待输入 A 或 E 两种消息 。如果收到 E 类消息，就执行 ...7 段代码，再因 break 跳出到 ...8 的位置。

如果收到 A 类消息，执行 ...2 代码，然后进程改为等待 B,C,D 类消息。此时，A,E 类消息都无效。直到收到 B 类消息后，执行流程到 ...6 并结束 A 消息的处理。再次等待输入 A 或 E 。

...8 后的阶段大致相同，但在 F 类消息的处理中，使用了并行的输入(fork listen) 。此时，系统会同时等待 G/H 和 F/I 的输入。只到有 H 类消息输入，中断 F 的处理流程，执行完 ...11 后，系统去掉 G/H 的输入等待。

要实现这么一套 DSL ，首先需要用已有的动态语言先实现一下所有功能，等需求稳定后，再设计 DSL 的语法。支持 coroutine 的 lua 非常适合做这件事情。

这套东西的框架其实是一个 coroutine 的调度器。每个执行流（就是 case message），不论是不是并行的，都是一个 coroutine 。当遇到 listen ，fork ，break 的时候 coroutine yield 出来，由调度器来决定下一步该 resume 哪个分支就好了。

框架只需要接收外界传入的带类型信息的 message ，在调度器里维护一张消息类型到执行流的映射表，就可以正确的调度这些东西。

剩下的事情就是处理穿插在其中的代码块内，数据相互之间可见性的问题；以及给 RPC 提供一个更易用的接口了。

我大约花了不到 100 行 lua 代码来实现以上提到的功能的雏形，贴在下面，尚需完善：

--- agent.lua
local setmetatable = setmetatable
local coroutine = coroutine
local assert = assert
local unpack = unpack
local print = print
local pairs = pairs

module "agent"

local function create_event_group(self, events, thread , parent_group)
    local group = {
        event = {},
        thread = thread,
        parent = parent_group,
    }
    if parent_group then
        for k,v in pairs(parent_group.event) do
            self.event[k] = nil
        end
    end

    for k,v in pairs(events) do
        group.event[k] = { func = v , group = group }
        assert(self.event[k] == nil , k)
        self.event[k] = group.event[k]
    end
end

local function pop_event_group(self, group)
    for k in pairs(group.event) do
        self.event[k] = nil
    end
    if group.parent then
        for k,v in pairs(group.parent.event) do
            assert(self.event[k] == nil , k)
            self.event[k] = v
        end
    end
end

function create(main)
    local mainthread = coroutine.create(main)
    local self = setmetatable( { event = {} } , { __index = _M })
    local r , command , events = coroutine.resume(mainthread , self)
    assert(r , command)
    assert(command == "listen" ,  command)
    create_event_group(self, events, mainthread)
    return self
end

function send(self, msg , ...)
    local event = self.event[msg]
    if event == nil then
        print (msg .. " unknown" , ...)
    else
        local event_thread = coroutine.create(event.func)
        local group = event.group
        while true do
            local r, command, events = coroutine.resume(event_thread, self, ...)
            assert(r,command)
            if command == "listen" then
                create_event_group(self, events, event_thread , group)
                break
            elseif command == "fork" then
                create_event_group(self, events, event_thread)
                break
            elseif command == "break" then
                pop_event_group(self, group)
                event_thread = group.thread
                group = group.parent
            else
                break
            end
        end
    end
end


function listen(agent , msg)
    coroutine.yield("listen",msg)
end

function quit(agent)
    coroutine.yield "break"
end

function fork(agent, msg)
    coroutine.yield("fork",msg)
end

--- test.lua

local agent = require "agent"

a = agent.create(function (self)
    self:listen {
        login = function (self)
            self:listen {
                username = function(self , ...)
                    print("username", ...)
                    self:listen {
                        password = function(self, ...)
                            print("password", ...)
                            self:quit()
                        end
                    }
                    self:quit()
                end ,
            }
        end,
        ok = function (self)
            self:quit()
        end,
    }
    print "login ok"
    local q = 0
    self:listen {
        chat = function (self, ...)
            print("chat", ...)
        end,
        question = function (self , ...)
            print("question", ...)
            local answer = "answer" .. q
            q = q+1
            self:fork {
                [answer] = function (self, ...)
                    print("answer", ...)
                    self:quit()
                end
            }
        end,
    }
end)

a:send("login")
a:send("username","alice")
a:send("username","bob")
a:send("password","xxx")
a:send("login")
a:send("username","bob")
a:send("password","yyy")
a:send("chat","foobar")
a:send("ok")

a:send("chat", "hello")
a:send("question","?0")
a:send("chat", "world")
a:send("question","?1")
a:send("answer0","!0")
a:send("answer1","!1")


12 月 5 日补充:

周一上班和蜗牛同学讨论了一下需求， 最后我们商定抽象出一个 session 出来， 供 fork 出来的流程使用。因为并行的流程会使用相同的消息类型，但流程上是独立的。

根据 session id 和 message type 做两级分发要清晰一些。

然后，我们再对 rpc 调用做简单的封装，使用更简单。

改进后的代码就不再列出了。
http://blog.codingnow.com/2011/12/dev_note_4.html

这周我开始做场景模块。因为所有 PC 在 server 端采用独立的 agent 的方式工作。他们分离开，就需要有一个模块来沟通他们。在一期目标中，就是一个简单的场景服务。用来同步每个 agent 看到的世界。

这部分问题，前不久思考过 。需求归纳如下：

    每个 agent 可以了解场景中发生的变化。
    当 agent 进入场景时，需要获取整个世界的状态。
    agent 进入场景时，需要可以查询到它离开时自己的状态。关于角色在场景中的位置信息由场景服务维护这一点，在 开发笔记1 中提到过。

大批量的数据同步对性能需求比较高。因为在 N 个角色场景中，同步量是 N*N 的。虽说，先设计接口，实现的优化在第二步。但是接口设计又决定了实现可以怎样优化，所以需要比较谨慎。

比如，同步接口和异步接口就是不同的。

请求回应模式最直观的方式就是设计成异步接口，柔韧性更好。适合分离，但性能不易优化。流程上也比较难使用。（使用问题可以通过 coroutine 技术来改善 ）即使强制各个进程（不一定是 os 意义上的进程）在同一个 CPU 上跑，绕开网络层，也很难避免过多的数据复制。虽有一些 zero-copy 的优化方案，但很难跨越语言边界。

同步接口使用简单，但在未经优化前，可能不仅仅是性能较差的问题，而是完全不可用。一旦无法优化，接口修改对系统的变动打击是很大的。（异步接口在使用时因为充分考虑了延迟的存在，及时没有优化，也不至于影响整个系统的运转）

在这次的具体问题上，我判断使用同步接口是性能可以接受的。即，用户可以直接查询场景的各种状态。蜗牛同学希望实现上更简单，干脆就是把所有 agent 以及未来的 npc 服务实现在一起，大家共享场景状态。因为框架使用 coroutine 调度，其实是相互没有冲突的。

在一个场景物理上在一个 CPU 上跑这个问题上，我没有不同意见。但是，我希望在设计上依旧独立。agent 的并发在实现上是不存在的，但在设计上是考虑进去的。agent 之间物理上完全独立，但在交互上使用一些优化手段达到零成本。

模块在物理上分离，是高健壮性和柔韧性的充分条件。最小依赖单一模块实现的质量。

至于性能开销，我的观点是，把所有东西塞到一起，使得沟通成本下降；比如大家都跑在同一 lua state 中，没有跨语言边界的信息传递；其实是一种成本延迟支付。抛开可能引起的健壮性下降问题，动态语言的 GC 代价也是初期无法预期的。

不过无论采用何种实现方案，接口设计总比其它更重要。需要留意的是，先分开实现再合起来，比先合在一起以后再分离，在实践操作中要难得多。这在历史上许多系统的大模块设计中被反复证明过：我看过太多上万行的 C++ 代码，几十个类绞在一起，留下飞线和后门，让类与类之间做必要的沟通。静态语言尚且如此，毋提动态语言这种随意性更高的东西了。

这周，怪物公司和小V 还在忙 client 那边跟美术的沟通和工具插件开发，原定的 C/S 联调一再被推迟。我自己在写了两三百行场景服务的 C 代码后，觉得可以暂放一下。所以最终接口还不需要完全敲定。前两天想到一些优化方案，也暂时不需要实现，先记录一下。

关于并发读场景状态的问题：

场景数据其实是允许读取方不那么严格需要一致的。因为场景数据是局部累积的一种缓慢变化。任何一个读方都不必一定读到最新的版本，而只要求读到一个完整的版本。即，我读到的场景是 0.05s 以前的状态也是可以被容忍的。0.05s 对于人类是一瞬间，但对于计算机却是一个相当长的时间段。

这种延迟本身也是存在的，因为更新场景的写入者本身也一定存在于不同机器上，对于网络通讯， 50ms 已经是一个不错的相应速度了。

如果在本地内存，做同步读取，任何一个需要原子性的读操作，需要的时间都远远低于 0.01s 数个数量级，这个速度差，让我们有一起契机摆脱并发操作需要的锁。

简单说，我想引入一个 triple buffer ，让本地内存中，场景数据有三份几乎相同的 copy 。为了后面好描述，我把它们称为 A B C 。

所有的场景更新者，通过一个消息队列提交更新需求，向 A B C 更新。每个更新请求，都需要依次更新完 A B C 后才销毁。

场景服务提供者控制一个写指针，每隔 0.01s 在 A B C 间翻转。也就是说，当 0.01s 的周期一到，下一次原子写操作就写向下一个场景镜像。

A B C 这三个场景镜像采用共享内存的方式，供唯一的一个写入队列以及不受限的读取者共享。写入队列维护进程在翻转写指针的同时，也翻转读指针。当写指针在 A 时，读指针在 C ；当写指针翻转到 B 时，读指针调整到 A ；等等。

每次读操作可以看成是原子的，操作前先获取目前的读指针指向的镜像，然后直接在这份镜像上做完后续的读操作。因为一次读操作需要消耗的时间远远低于 0.01s ，而写指针触碰回这个镜像的时间则在 0.01s 到 0.02s 之间，不会短于 0.01s 。这样，绝对不会发生读写冲突。

当然，用 double buffer 也是可行的，但需要在读取完毕后重新检查一下当前的读指针，看看是否还指向原来的镜像，如果已经切换了，就需要放弃刚才的结果，重新读一次。而且在读过程中，需要保证错误的数据（可能因为正在写入而不完整）不会引起程序崩溃。这样开发难度会增加。

事后复审读指针依旧可以做，如果发生异常记入 log ，一旦发生再来考量时间窗口的大小设置是否合理，或是看看到底什么意外导致了读写冲突。

对于高负载下，读操作可能被挂起，引起的原子性被破坏的问题，其实也是可测算的。

首先，在读 api 中，应该避免任何 IO 操作以及系统调用，比如写 log 。一切 log 都应该在操作完毕后再统一进行。一次原子读操作中，需要保证是单纯的 CPU/内存 操作指令。这样可以保证单一 api 的过程，最多被 os 调度器打断一次。（因为指令数足够少，远小于 os 的最小时间片跑的 cpu 指令数）

写操作的间隔时间足够长，如果调度器是公平的，就可以保证在同样条件下写操作的镜像翻转消耗的时间片大于两倍的单次读操作的时间片。这样，做两次翻转后，保证一次读操作至少经历了同样的两倍以上的时间片。可以认为，单次读操作不会被写操作损坏。
http://blog.codingnow.com/2011/12/dev_note_5.html

开始这个话题前，离上篇开发笔记已经有一周多了。我是打算一直把开发笔记写下去的，而开发过程中一定不会一帆风顺，各种技术的抉择，放弃，都可能有反复。公开记录这个历程，即是对思路的持久化，又是一种自我督促。不轻易陷入到技术细节中而丢失了产品开发进度。而且有一天，当我们的项目完成了后，我可以对所有人说，看，我们的东西就是这样一步步做出来的。每个点滴都凝聚了叫得上名字的开发人员这么多个月的心血。

技术方案的争议在我们几个人内部是很激烈的。让自己的想法说服每个人是很困难的。有下面这个话题，是源于我们未来的服务器的数据流到底是怎样的。

我希望数据和逻辑可以分离，有物理上独立的点可以存取数据。并且有单独的 agent 实体为每个外部连接服务。这使得进程间通讯的代价变得很频繁。对于一个及时战斗的游戏，我们又希望对象实体之间的交互速度足够快。所以对于这个看似挺漂亮的方案，可能面临实现出来性能不达要求的结果。这也是争议的焦点之一。

我个人比较有信心解决高性能的进程间数据共享问题。上一篇 谈的其实也是这个问题，只是这次更进一步。

核心问题在于，每个 PC (玩家) 以及有可能的话也包括 NPC 相互在不同的实体中（我没有有进程，因为不想被理解成 OS 的进程），他们在互动时，逻辑代码会读写别的对象的数据。最终有一个实体来保有和维护一个对象的所有数据，它提供一个 RPC 接口来操控数据固然是必须的。因为整个虚拟世界会搭建在多台物理机上，所以 RPC 是唯一的途径。这里可以理解成，每个实体是一个数据库，保存了实体的所有数据，开放一个 RPC 接口让外部来读写内部的这些数据。

但是，在高频的热点数据交互时，无论怎么优化协议和实现，可能都很难把性能提升到需要的水平。至少很难达到让这些数据都在一个进程中处理的性能。

这样，除了 RPC 接口，我希望再提供一个更直接的 api 采用共享状态的方式来操控数据。如果我们认为两个实体的数据交互很频繁，就可以想办法把这两个实体的运行流程迁移到同一台物理机上，让同时处理这两个对象的进程可以同时用共享内存的方式读写两者的数据，性能可以做到理论上的上限。

ok, 这就涉及到了，如何让一块带结构的数据被多个进程共享访问的问题。结构化是其中的难点。

方案如下：

我们认为，需要交互和共享的数据，就是最终需要持久化到外存中的数据。整体上看，它好像一个小型内存数据库。它一定可以通过类似 google protocol buffers 的协议来序列化为二进制流。它和内存数据结构是有区别的。主要是一些约束条件，让这件事情可以简单点解决，又能满足能想到的各种需求。

数据类型是有限的：

    nil
    number
    boolean
    string
    map
    array

以上 6 种类型足够描述所有的需求，这在 lua 中已得到了证实。不过这里把 lua 的 table 拆分为 map 和 array 是对 protobuf 的一种借鉴。这里的 map 是有 data scheme 的，而不是随意的字典。即 key 一定是事先定义好的原子，在储存上其实是一个整数 id ，而 value 则可以是其它所有类型。

array 则一定是同类型数据的简单集合，且不存在 array 的 array 。这种方式的可行性在 protobuf 的应用中也得到了证实。

本质上，任何一个实体的所有数据，都可以描述为一个 map 。也就是若干 key-value 对的集合。array 只是相同 key 的重复（相当于 protobuf 里的 repeated）。

这里可以看出，除了 string 外，所有的 value 都可以是等长的，适合在 C 里统一储存。每个条目就是 id - type - value - brother 的一组记录而已。

其中 map 用二叉树的方式储存就可以满足节点的定长需求，左子树是它的第一个儿子，右子树是它的兄弟。

我们用一个固定内存块来保存整块数据，里面都是等长的记录，map 的记录中，左右子树都用保存着全局记录序号。

string 需要单独储存，所有的 string 都额外保存在另一片内存中（也可以是同一片内存的另一端）。在记录表中，记录 string 内容在 string pool 中的位置。

这样做有什么好处？

由于数据有 scheme（可以直接用 protobuf 格式描述），所以数据在每个层次上的规模是可以预估的，数据都是以等长记录保存的，对整个数据块的修改都可以看成是对局部数据的修改或是对整体的追加。这两个操作恰巧都可以做成无锁的操作。

换句话说，每次对整颗树具体一个节点的修改，都绝对不会损坏其它节点的数据。

有了这块组织好的数据结构有什么用呢？首先持久化问题就不是问题，但这只是一个附带的好处。这块数据虽然能完整的记录各种复杂的结构数据，但不利于快速检索。我们需要在对这颗树的访问点，制作一个索引结构。如果导入到 lua 中，就是一个索引表。当我们第一次需要访问这颗树的特定节点：体现为读写 xxx.yyy.zzz 的形式，我们遍历这颗树，可以方便的找到节点的位置。大约时间复杂度是 O(N^2) ：要遍历 N 个层次，每个层次上要遍历 M 个节点。当然这里 N 和 M 都很小。

一旦找到节点的位置，我们就可以在 lua 中记录下这个绝对位置。因为每个节点一旦生成出来，就不会改变位置了。下次访问时，可以通过这个位置直接读写上面的数据了。

string 怎么办呢？我的想法是开一个 double buffer 来保存 string 。string 和节点是一一对应的关系。当节点上的 string 修改时，就新增加一个 string 到 pool 里，并改变引用关系。当一个 string pool 满后，可以很轻易的扫描整个 string pool ，找到那些正在引用的 string ，copy 到另一个 string pool 中。这个过程比一般的 GC 算法要简单的多。

最后就是考虑读写锁的问题了。只有一些关键的地方需要加锁，而大部分情况下都可以无锁处理。甚至在特定条件下，整个设计都可以是无锁的。

btw, 这一周剩下来的时间就是实现了。多说无益，快速实现出来最有说服力。按照惯例，应该会开源。
http://blog.codingnow.com/2011/12/dev_note_6.html

开发笔记 (9) ：近期工作小结
http://blog.codingnow.com/2012/01/dev_note_9.html	

开发笔记 (10) ：内存数据库
http://blog.codingnow.com/2012/02/dev_note_10.html
